{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import keras.layers as kl\n",
    "import keras.backend as K\n",
    "import keras.callbacks as kc\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, load_model, Model\n",
    "\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_data : charge et prépare les données"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "serie_length : combien de valeur seront utilisées dans l'input du modèle\n",
    "to_predict : le nombre de \"serie_length\" de temps dans le futur à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, serie_length, to_predict = 1, overlap = False):\n",
    "\n",
    "    # Identity sensor from filename\n",
    "    sensor = os.path.basename(filename).split('.')[1]\n",
    "    assert sensor in ['co2','hum','lum','temp']\n",
    "\n",
    "    # to get inputs whatever the OS\n",
    "    data = pd.read_csv(filename, sep= \";\")\n",
    "\n",
    "    data.head()\n",
    "\n",
    "    # Prepare data\n",
    "\n",
    "    values = data[sensor].values\n",
    "    timestamps = data[\"timestamp\"].values\n",
    "\n",
    "    # remove beginning\n",
    "    nb_todelete = len(values) % serie_length\n",
    "    values = values[nb_todelete:]\n",
    "    timestamps = timestamps[nb_todelete:]\n",
    "\n",
    "    # create X and Y\n",
    "    # X.shape = (n, serie_length)   X[i,j] = values[i+j]\n",
    "    # Y.shape = (n,)                Y[i] = mean( X[i+to_predict,:] )\n",
    "    # n = len(values) - (serie_length * to_predict)\n",
    "    if overlap:\n",
    "        step = 1\n",
    "    else:\n",
    "        step = serie_length\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(values) - (serie_length * to_predict), step):\n",
    "        window = values[i : i + serie_length]\n",
    "        next_window = values[i + serie_length * to_predict : i + serie_length * (to_predict + 1)]\n",
    "        X.append(window)\n",
    "        Y.append(next_window.mean())\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    timestamps = np.array(timestamps[serie_length * to_predict::step])\n",
    "\n",
    "    # Normalisation entre min et max vers des valeurs de 0 à 1\n",
    "    Yminmax = Y.min(), Y.max()\n",
    "    X = np.interp(X, (X.min(), X.max()), (0, 1))\n",
    "    Y = np.interp(Y, (Y.min(), Y.max()), (0, 1))\n",
    "\n",
    "    return X, Y, timestamps, Yminmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_data : partitionne les ensembles train, dev et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, timestamps, ratio_train_dev=0.8, ratio_dev_test=0.1):\n",
    "    # split between training and validation\n",
    "    ratio_train_dev = 0.8\n",
    "    ratio_dev_test = 0.1\n",
    "    split_index_1 = int(len(X) * ratio_train_dev)\n",
    "    split_index_2 = int(len(X) * (ratio_train_dev + ratio_dev_test))\n",
    "\n",
    "    X_train, X_dev, X_test = X[ : split_index_1], X[split_index_1 : split_index_2], X[split_index_2 : ]\n",
    "    Y_train, Y_dev, Y_test = Y[ : split_index_1], Y[split_index_1 : split_index_2], Y[split_index_2 : ]\n",
    "    timestamps_test = timestamps[split_index_2:]\n",
    "    \n",
    "    # LSTM wants 3d vectors\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_dev = np.expand_dims(X_dev, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    \n",
    "    return (X_train, Y_train), (X_dev, Y_dev), (X_test, Y_test), timestamps_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK USING KERAS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "l'objet KerasModel a trois fonctions : fit, predict et score\n",
    "la définition du réseau de neurone à proprement parler est dans la partie \"fit\"\n",
    "\n",
    "il se compose d'une couche d'entrées de la taille \"serie_length\".\n",
    "Etant donnée que les données sont une série temporelle, nous avons fait le choix d'un réseau récurrent (RNN), au niveau de la première couche nous avons essayer \"LSTM\" et \"GRU\" (dans l'exemple la couche se compose de 64 neurones \"LSTM\").\n",
    "\n",
    "Une couche cachée de 16 neurones à activation \"ReLu\" est utilisée pour faire une première regression linéaire et enfin un neurone de sortie à activation \"sigmoid\". Beaucoup de nos choix (comme la taille du batch) ont été faite de manière empirique puisqu'on manque encore de théorie (et surtout de pratique) au niveau de la conception d'un modèle en situation réelle.\n",
    "\n",
    "L'entrainement est assez long, environ 3 minutes pour une vingtaine d'epochs ce qui a ralenti la réalisation de tous les tests que nous voulions faire. Pour améliorer les performances de l'entrainement, nous avons certains des \"callbacks\" de Keras tels que \"ReduceLRonPlateau\" et \"ModelCheckpoint\" pour pouvoir enregistrer le meilleur modèle trouvé, ce qui a été très pratique.\n",
    "\n",
    "L'enregistrement d'un modèle se fait sous le format \"Hierarchical Data\" (.h5) utilisée pour les fichiers contenant un important volume de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du modèle\n",
    "![Image du modèle non trouvée](model_plot.png \"model plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModel(object):\n",
    "\n",
    "    def __init__(self, serie_length, to_predict, name = \"model-dev\"):\n",
    "        self.serie_length = serie_length\n",
    "        self.to_predict = to_predict\n",
    "        self.model_name = \"models/{}.h5\".format(name)\n",
    "\n",
    "\n",
    "    def fit(self, X_train, Y_train, X_dev, Y_dev, compile_model = True, save_path = None):\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = self.model_name\n",
    "\n",
    "        if compile_model:\n",
    "            # Recusive way to define RNN\n",
    "            neural_network_input = kl.Input(X_train[0].shape)\n",
    "\n",
    "            recursive_neural_network = kl.LSTM(64)(neural_network_input)\n",
    "            # recursive_neural_network = kl.GRU(64)(neural_network_input)\n",
    "            # recursive_neural_network = kl.CuDNNGRU(64)(neural_network_input) # for nvidia (way faster)\n",
    "\n",
    "            dense = kl.Dense(16, activation = \"relu\")(recursive_neural_network)\n",
    "            dense = kl.Dense(1, activation = \"sigmoid\")(dense)\n",
    "\n",
    "            model = Model(inputs = [neural_network_input], outputs = [dense])\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss = \"mse\", optimizer = \"adam\")\n",
    "\n",
    "            # permet d'arrêter le training avant la fin si rien n'est améliorer en 10 epochs\n",
    "            early_stop = kc.EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 1, mode = 'min')\n",
    "            # enregistre le modèle si le val_loss est meilleur que l'actuel meilleur modèle\n",
    "            checkpoint = kc.ModelCheckpoint(save_path, monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "            # permet de revenir en arrière si rencontre avec un min local en reduisant le LR\n",
    "            reduce_lr = kc.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.25, patience = 25, verbose = 1, mode = 'min', min_lr = 1e-6)\n",
    "\n",
    "            callbacks = [early_stop, checkpoint, reduce_lr]\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, Y_train,\n",
    "                validation_data = (X_dev, Y_dev),\n",
    "                epochs = 20,\n",
    "                batch_size = 16,\n",
    "                callbacks = callbacks\n",
    "            )\n",
    "            \n",
    "        self.model = load_model(save_path)\n",
    "  \n",
    "        plot_model(self.model, to_file = 'model_plot.png', show_shapes = True, show_layer_names = True)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        return self.model.evaluate(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(object):\n",
    "    def __init__(self, serie_length, to_predict):\n",
    "        self.serie_length = serie_length\n",
    "        self.to_predict = to_predict\n",
    "\n",
    "        self.model = linear_model.LinearRegression()\n",
    "\n",
    "    def fit(self, X_train, Y_train, X_dev, Y_dev):\n",
    "        self.model.fit(X_train.reshape((-1, self.serie_length)), Y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X.reshape((-1, self.serie_length)))\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        return ((self.predict(X) - Y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ilot1.temp.300.csv\"\n",
    "path = os.path.join(os.path.dirname(\"iter3\"), \"..\", \"data\", \"output\", filename)\n",
    "serie_length = 15\n",
    "to_predict = 1\n",
    "name = \"model-dev\"\n",
    "compile_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, timestamps, Yminmax = load_data(path, serie_length, to_predict)\n",
    "(train_set1, train_set2), (dev_set1, dev_set2), test_set, timestamps = split_data(X, Y, timestamps, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = KerasModel(serie_length, to_predict, name)\n",
    "m_linear = LinearModel(serie_length, to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement sur les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                16896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 17,953\n",
      "Trainable params: 17,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4096 samples, validate on 512 samples\n",
      "Epoch 1/20\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01072, saving model to models/model-dev.h5\n",
      "Epoch 2/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 8.2486e-04 - val_loss: 0.0060\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01072 to 0.00605, saving model to models/model-dev.h5\n",
      "Epoch 3/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 7.0931e-04 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00605 to 0.00526, saving model to models/model-dev.h5\n",
      "Epoch 4/20\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 6.6655e-04 - val_loss: 0.0042\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00526 to 0.00416, saving model to models/model-dev.h5\n",
      "Epoch 5/20\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 5.7020e-04 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00416 to 0.00284, saving model to models/model-dev.h5\n",
      "Epoch 6/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 5.7960e-04 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00284\n",
      "Epoch 7/20\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 5.0817e-04 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00284 to 0.00227, saving model to models/model-dev.h5\n",
      "Epoch 8/20\n",
      "4096/4096 [==============================] - 4s 933us/step - loss: 4.0960e-04 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00227 to 0.00219, saving model to models/model-dev.h5\n",
      "Epoch 9/20\n",
      "4096/4096 [==============================] - 4s 929us/step - loss: 3.9575e-04 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00219\n",
      "Epoch 10/20\n",
      "4096/4096 [==============================] - 4s 965us/step - loss: 3.4258e-04 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00219 to 0.00168, saving model to models/model-dev.h5\n",
      "Epoch 11/20\n",
      "4096/4096 [==============================] - 4s 992us/step - loss: 3.2919e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00168 to 0.00129, saving model to models/model-dev.h5\n",
      "Epoch 12/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 2.9466e-04 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00129\n",
      "Epoch 13/20\n",
      "4096/4096 [==============================] - 6s 1ms/step - loss: 3.0115e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00129 to 0.00121, saving model to models/model-dev.h5\n",
      "Epoch 14/20\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 2.8024e-04 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00121\n",
      "Epoch 15/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 2.6357e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00121 to 0.00106, saving model to models/model-dev.h5\n",
      "Epoch 16/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 2.4943e-04 - val_loss: 9.8334e-04\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00106 to 0.00098, saving model to models/model-dev.h5\n",
      "Epoch 17/20\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 2.5875e-04 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00098\n",
      "Epoch 18/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 2.6310e-04 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00098\n",
      "Epoch 19/20\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 2.6880e-04 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00098\n",
      "Epoch 20/20\n",
      "4096/4096 [==============================] - 4s 994us/step - loss: 2.6472e-04 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00098\n"
     ]
    }
   ],
   "source": [
    "m.fit(train_set1, train_set2, dev_set1, dev_set2, compile_model = compile_model)\n",
    "m_linear.fit(train_set1, train_set2, dev_set1, dev_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LinearRegression :', 0.00022511491707237317)\n",
      "512/512 [==============================] - 0s 791us/step\n",
      "('KerasModel MSE :', 0.004533971783530433)\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = test_set\n",
    "Y_test_pred = m.predict(X_test)\n",
    "Y_linear_pred = m_linear.predict(X_test)\n",
    "\n",
    "print(\"LinearRegression :\", m_linear.score(X_test, Y_test))\n",
    "print(\"KerasModel MSE :\", m.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timestamps, np.interp(Y_test, (0,1), Yminmax),\n",
    "    label=\"truth\", alpha=0.8)\n",
    "plt.plot(timestamps, np.interp(Y_linear_pred, (0,1), Yminmax), \n",
    "    '--', label=\"linear\", alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"timestamp\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.plot(timestamps, np.interp(Y_test, (0,1), Yminmax),\n",
    "    label=\"truth\", alpha=0.8)\n",
    "plt.plot(timestamps, np.interp(Y_test_pred, (0,1), Yminmax), \n",
    "    '--', label=\"keras prediction\", alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"timestamp\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
